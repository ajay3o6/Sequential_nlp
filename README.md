# Sequential_nlp

Generateing Word Embeddings and retrieve outputs of each layer with Keras based
on the Classification task.
Word embedding are a type of word representation that allows words with
similar meaning to have a similar representation.
It is a distributed representation for the text that is perhaps one of the key
breakthroughs for the impressive performance of deep learning methods on
challenging natural language processing problems.
The IMDb dataset is used to learn word embedding as we train our dataset.
This dataset contains 25,000 movie reviews from IMDB, labeled with a sentiment
(positive or negative)
